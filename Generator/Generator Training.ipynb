{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 635,
     "referenced_widgets": [
      "f1e359c4ec6a44629031df1421457a4e",
      "ea0452d9e83e4eb7a5cf7f96c6a38dbf",
      "dc5f90be653b4419b5190e5030eea091",
      "1c56557cfb9e4384a13a8ccb1ccf57b0",
      "331a7a8fc3884ba29114b565c0119e68",
      "20afe866f8a84b06aee9d80d1d29648b",
      "3fe4bc232a8644f7884d07dbb2004392",
      "a759b5a28d8f441ca160f9ae28366e00",
      "6d8bfe8876524aa4932bb1fd4ccbafc0",
      "498a5387ee7442f69178c6324e005596",
      "16f432e8470940ceae3015bf47afb9f3",
      "92065a8d25e74e7bb93f334e87094b18",
      "e71ed7613129406582b8419f1634c916",
      "ff75be22829d43728f602f7557260637",
      "a406aae0f1e84fc39a6821069cad3ca6",
      "2849d03c3b9c41fcaac0eb6ada02a24c",
      "24afc14243d2405885e2c62fd28653a1",
      "a7508a601d464cedba61ca2b88d3b376",
      "ae4327a4c50d4a258ab8cbd9d8c00fce",
      "c97ee737afdc4f37a905ba11c700e1e6",
      "8856c14c9b7b4380872e68b373cf90b6",
      "e0cd56d2b36444c6a34d8bb0fe332594",
      "0ab43378cb24468799cde11daf07b141",
      "6f414da014084b83bb678ca976a8ed8b",
      "ef5156a8ef9949328adc4f1bf35a9c75",
      "bc4f0d01ea9a4db88c508438cded68be",
      "9eb96705d8e44c08b2e1fe610f9f56c7",
      "c39f2c3e1e894836bf2e9db2b9bd2e07",
      "6cd8fa6900ac4290b22061c5aa9c03ee",
      "141c77bb3eca455894c301599f4aed4f",
      "33e0e4c507b94d9da1958fe8e759710e",
      "986a23c90ce545c69f5bbca3acf786b4",
      "e0b9d58acadb4807bb76645e1dd6f127",
      "5932365eeb4b470e89ddf211721da609",
      "d56e33aeb70447cd8c0f587c915f90a7",
      "8f334f4830114d848f0e13bb47428ffe",
      "c9a7940edac14c809a52cdacb23a2b93",
      "d81fac668e504944beb1e3a13c93124f",
      "ad218c9566e84a26927fafa2b21bfcc4",
      "740607b6e216402abd4d279ad3baa5d1",
      "b8a082a5d370451f99e0f2d0910fe09e",
      "59ef9acd1274487393a42ea3064a60c6",
      "81caf23fa6424a588a33fbd93628f352",
      "b004ef305f274f1681a4d014ffcfd0dc",
      "81befb3d7f2c4209a005900daf07b5cc",
      "6724c8e946c741bbb1603baf7c97fdfd",
      "fe3a03b288ef452c909bb71bae833fa0",
      "5f8dd54fbc924291b734094a610fc8e5",
      "1f77fe551c974e338968fc89957acdbf",
      "de9d378cd73948978869558b2603d634",
      "5353475afa1f41a9a7bf747dfe4685b2",
      "0809c6dd7056478f9291d119cee39df1",
      "928c309c2cb94a2d8b1998bae7ef0f17",
      "436b3d98ee374a85b3c7943da6090a07",
      "2b94af7a316e4a5f9c6e6d039871936c",
      "ddd82dbd730c4f168796efd4b472b303",
      "a67716ba792f42dbaae1440b0bd519f9",
      "3c3225f569384ba28571d48acca3b1c3",
      "6699cfda584642c28a58554c2b54fb6e",
      "125aff6bdadc4c8287bade7c43a60229",
      "602da426a24a4b4187d86b7c15f23089",
      "144a41d8f05d4200b45d296bed3a1844",
      "37d951a4c9b74b40b9d3ff0cff2826ab",
      "2f012b79a90f4e76ae3f9c9c0c6a0b5d",
      "1a2a0b62b37146ac96ea85ba38a07559",
      "9c1006ab491e4aaea5801a672e6d02d4",
      "32a8e615c33d40c999c9bc936ea0790d",
      "6b91cf0fb1d24dae9b1e4fd604fb37b3",
      "7255d21365264dd28a9bbef08213aa26",
      "cc90ab47d3d9476c9e0b6c2083dda5b4",
      "d36e20ed2b524366b29b72bac1bd3852",
      "5e51364efbd24cdc9b40973afefaabf3",
      "e9a4365d85504b4fb6cdbc04cecb7818",
      "7f1f57593b81454995c808e40228cb23",
      "bdd48626a9884191a738a778819265de",
      "80dcb9fd92c141f79208dd219b15602a",
      "d55d27332ace4a1498d2db87bf184d82",
      "024c5f961fdc41a989fe96fbdb1e2d50",
      "1c2831b6cfe54acdbd03b4c369530231",
      "3b525e9619bb416e959e73c5889426b6",
      "ff312dba01154f73971c7700758823d8",
      "e3b9bd69ec0b4f0ead2f6f3cd3739bb9",
      "79bac6fbb2e64f0386173bae23540a3c",
      "491f9af85b884438a93825cd52afeeec",
      "c36fb3f3063a4a9fa3970b1487236513",
      "16b475fe639c4d3b974e11ccbe1f434c",
      "dd5d01538f3b44b6b24efd979cfcedd6",
      "b0093e13d93f46fda4d9eb3f8d38c19e",
      "bdbc78d1a046494e8a671eb5301cb9fa",
      "543b2d5da59448f3a5a753034a6ebdf9",
      "ca933227a1af429daa851da17686da84",
      "9d520e10f78b4a13aaa606101cf37229",
      "263d4c400eac4bb59fd51cafeb9244e2",
      "f9fc302d15dc4fad9d443ba040437e05",
      "08935d9b4ea942b8ab14d758a63bf668",
      "2674bb9f10c0408db7b36460c364c973",
      "c96d09bea8eb4b3381863e1f4278b5d7",
      "1a6ca148e9374ad5bb5c4dc4b2cbda58",
      "59709ccd629f4278b619ed0bc2d79bf0",
      "e5259ad79dca4d869af8e06eb8af8275",
      "06153ca5e66445f993bff6e488e71a51",
      "4dcb73477c7149ca8f55865906b5ca0d",
      "df8b4baa4e354814886728edb78f47bb",
      "1f3c1fa48abc4f51bb3fda73b0154b2b",
      "d7576b457e0e4448a0b639f27a1cc45f",
      "d8b722853a79431cb4a5068cfe3b32be",
      "b1f82c79acba4d1f90d59da2fc7cc776",
      "950e692bb92a4def9ace1e504af4af08",
      "585937c9ef8b49b88d217d71fc591a97",
      "cdf3cd71ad9d49548bbdd904d764802b"
     ]
    },
    "id": "F5wD4Zo60lBb",
    "outputId": "63bab27d-ec0c-4473-8d96-e7f3e7366e75"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import load_from_disk  # or load_dataset if remote\n",
    "import re\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "MODEL_NAME = \"t5-small\"\n",
    "MAX_INPUT_LENGTH = 512\n",
    "MAX_TARGET_LENGTH = 128\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 3\n",
    "LR = 3e-5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# -----------------------------\n",
    "# Load Dataset\n",
    "# -----------------------------\n",
    "full_dataset = load_from_disk(\"masked_dataset\")\n",
    "\n",
    "# split train test\n",
    "train_test = full_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = train_test[\"train\"]\n",
    "test_dataset = train_test[\"test\"]\n",
    "\n",
    "dataset = train_dataset\n",
    "\n",
    "def remove_mask_content(example):\n",
    "    text = example[\"input_text\"]\n",
    "    example[\"input_text\"] = re.sub(r\"\\[MASK_START\\].*?\\[MASK_END\\]\", \"[MASK_START][MASK_END]\", text, flags=re.DOTALL)\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(remove_mask_content)\n",
    "test_dataset = test_dataset.map(remove_mask_content)\n",
    "\n",
    "# Option 2: or if it’s in memory already:\n",
    "# from datasets import Dataset\n",
    "# dataset = Dataset.from_dict({\"input_text\": [...], \"target_text\": [...]})\n",
    "\n",
    "# -----------------------------\n",
    "# Load tokenizer and model\n",
    "# -----------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "\n",
    "# -----------------------------\n",
    "# Tokenize function for datasets.map()\n",
    "# -----------------------------\n",
    "def preprocess_function(batch):\n",
    "    model_inputs = tokenizer(\n",
    "        batch[\"input_text\"],\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch[\"target_text\"],\n",
    "            max_length=MAX_TARGET_LENGTH,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "    # Replace pad token IDs with -100 so they’re ignored in cross-entropy loss\n",
    "    labels[\"input_ids\"] = [\n",
    "        [(token if token != tokenizer.pad_token_id else -100) for token in label]\n",
    "        for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# -----------------------------\n",
    "# Apply preprocessing\n",
    "# -----------------------------\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names,\n",
    ")\n",
    "\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "tokenized_test = test_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=test_dataset.column_names,\n",
    ")\n",
    "\n",
    "tokenized_test.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(tokenized_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(tokenized_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "# -----------------------------\n",
    "# Optimizer\n",
    "# -----------------------------\n",
    "optimizer = AdamW(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 5169/5169 [18:23<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 | Avg Loss: 1.9718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 5169/5169 [18:06<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 | Avg Loss: 1.7341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 5169/5169 [18:03<00:00,  4.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 | Avg Loss: 1.6260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# -----------------------------\n",
    "# Training loop\n",
    "# -----------------------------\n",
    "model.train()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    # tqdm progress bar for batches\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\", leave=True)\n",
    "\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Avg Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./model1\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Save model & tokenizer\n",
    "# -----------------------------\n",
    "SAVE_PATH = \"./model1\"\n",
    "\n",
    "model.save_pretrained(SAVE_PATH)\n",
    "tokenizer.save_pretrained(SAVE_PATH)\n",
    "print(f\"Model and tokenizer saved to {SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TA9AlVijukNE",
    "outputId": "4b3d3a55-d940-4de1-ac3b-86508b1b2d8a"
   },
   "outputs": [],
   "source": [
    "# Example of test prediction\n",
    "\n",
    "def generate_masked_span(model, tokenizer, input_text, max_new_tokens=128):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True).to(DEVICE)\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=250,\n",
    "        min_new_tokens=30,\n",
    "        num_beams=5,\n",
    "        length_penalty=1.4,\n",
    "        repetition_penalty=1.1,\n",
    "        no_repeat_ngram_size=3,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "'''\n",
    "| Category                  | Parameters                                                                  | Purpose                            |\n",
    "| ------------------------- | --------------------------------------------------------------------------- | ---------------------------------- |\n",
    "| **Length control**        | `max_new_tokens`, `min_new_tokens`, `length_penalty`                        | Controls output size               |\n",
    "| **Quality (beam search)** | `num_beams`, `length_penalty`, `no_repeat_ngram_size`, `repetition_penalty` | Improves coherence, avoids loops   |\n",
    "| **Creativity (sampling)** | `do_sample`, `temperature`, `top_p`                                         | Adds randomness and variation      |\n",
    "| **Structure**             | `**inputs`                                                                  | Provides prompt and attention mask |\n",
    "'''\n",
    "\n",
    "# -----------------------------\n",
    "# Example inference\n",
    "# -----------------------------\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "SAVE_PATH = \"./model1\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(SAVE_PATH).to(DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(SAVE_PATH)\n",
    "test_example = test_dataset[0][\"input_text\"]\n",
    "predicted = generate_masked_span(model, tokenizer, test_example)\n",
    "print(\"\\n--- Predicted masked section ---\\n\")\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LFyTYYJBCJG0",
    "outputId": "d95620bc-37d0-48b8-8152-40cf48abbd0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Input Example ---\n",
      "\n",
      "<DELETE> <START_OUTLINE> <BOE> Amy discovers a big envelope on the table while preparing a gift for her friend Tom's birthday. <EOE> <BOE> Amy feels happy with the wrapped gift, but her mom is upset when she sees it. <EOE> <BOE> Amy's mom explains that the envelope contained important papers needed for her work. <EOE> <BOE> Feeling guilty, Amy helps her mom unwrap the gift to retrieve the important papers. <EOE> <BOE> They return the papers to the envelope and find another way to wrap Tom's gift. <EOE> <BOE> Through this experience, Amy learns the importance of listening to her mom and being obedient. <EOE> <END_OUTLINE> <START_STORY> One day, a little girl named Amy found a big envelope on the table. She wanted to wrap a gift for her friend Tom's birthday. [MASK_START][MASK_END] She was very happy with how it looked. But when her mom saw the wrapped gift, she was not happy. The envelope had important papers inside for her work. They needed to find the papers before her mom's meeting. Amy felt bad for not being obedient. She and her mom unwrapped the gift and found the papers. They put the papers back in the envelope and found a different way to wrap Tom's gift. Amy learned to listen to her mom and be more obedient. <END_STORY>\n",
      "\n",
      "--- Predicted Masked Sentence ---\n",
      "\n",
      "One day, Amy found a big envelope on the table. She wanted to wrap the gift for her friend Tom's birthday. She asked her mom, \"Can I wrap my gift?\" Her mom smiled and said, \"Yes, I will wrap your gift.\" Amy and her mom were very happy.\n",
      "\n",
      "--- Target Sentence ---\n",
      "\n",
      "Amy's mom told her to be obedient and not touch the envelope. But Amy thought it would be perfect for wrapping Tom's gift. Amy took the envelope and wrapped Tom's gift inside.\n",
      "\n",
      "--- Input Example ---\n",
      "\n",
      "<ADD> <START_OUTLINE> <BOE> Leo, a little leopard, lived in a big forest with his family and friends. <EOE> <BOE> Leo and his friends played together every day, enjoying their time in the forest. <EOE> <BOE> One day, Leo discovered a big stick that he thought could help him reach high tree leaves. <EOE> <BOE> Leo showed the stick to his friends, and they all found different ways to use it. <EOE> <BOE> The stick became a shared resource, making all of Leo's friends happy and thankful. <EOE> <BOE> Leo felt proud of helping his friends, and they all lived happily ever after. <EOE> <END_OUTLINE> <START_STORY> Once upon a time, there was a little leopard named Leo. Leo lived in a big forest with his family. He had a lot of friends, like birds, monkeys, and elephants. They all played together and had fun every day. One day, Leo found a big stick on the ground. He thought it could be useful to help him reach the high tree leaves. [MASK_START][MASK_END] Leo was happy that the stick belonged to everyone in the forest. They all shared it and took turns using it. The useful stick made all of Leo's friends happy, and they thanked him for finding it. Leo felt proud that he could help his friends, and they all lived happily ever after. <END_STORY>\n",
      "\n",
      "--- Predicted Masked Sentence ---\n",
      "\n",
      "Leo showed the stick to his friends, and they all found different ways to use it. The stick became a shared resource, making all of Leo's friends happy and thankful.\n",
      "\n",
      "--- Target Sentence ---\n",
      "\n",
      "He showed it to his friends, and they all tried to use it. The birds used it to build a nest, the monkeys used it to play, and the elephants used it to scratch their backs.\n",
      "\n",
      "--- Input Example ---\n",
      "\n",
      "<ADD> <START_OUTLINE> <BOE> A deaf little girl discovers a trunk in the attic and decides to hide something important in it. <EOE> <BOE> In her room, she opens the trunk and finds a special toy mouse, which she decides to hide away. <EOE> <BOE> She places the toy mouse deep inside the trunk and closes it tightly, feeling happy about its safety. <EOE> <BOE> The girl runs downstairs to show her mom the toy, who encourages her to keep it safe in the trunk. <EOE> <BOE> Her mom reminds her that whatever she hides must be safe forever, which makes the girl content. <EOE> <END_OUTLINE> <START_STORY> Once upon a time, a deaf little girl found a trunk in the attic. She wanted to hide something important in it, so she grabbed it and ran to her room. Once she was in her room, she opened the trunk and peeked inside. She found a little toy mouse, which was so special that she decided to hide it away. She placed it deep inside the trunk, and closed it tightly with a click. The little girl was very happy that the little mouse was safe and sound, so she decided to find some friends to show it to. [MASK_START][MASK_END] But remember, whatever you hide must be safe in there forever.\" The little girl was content. She ran to her room and put the little mouse safely back into the trunk, where it would stay forever. <END_STORY>\n",
      "\n",
      "--- Predicted Masked Sentence ---\n",
      "\n",
      "The little girl ran downstairs to show her mom the toy. Her mom said, \"Don't worry, let's keep it safe in the trunk.\n",
      "\n",
      "--- Target Sentence ---\n",
      "\n",
      "She ran downstairs and saw her mom. Excitedly, she said, \"Mom, look what I found!\" Her mom smiled and said, \"That looks like a very special toy!. What are you going to do with it?\" The little girl said, \"I'm going to hide it in the trunk, so nobody can find it!\" Her mom nodded and said, \"That sounds like a great idea.\n",
      "\n",
      "--- Input Example ---\n",
      "\n",
      "<ADD> <START_OUTLINE> <BOE> An incredible goose flies high in the sky, exploring the world below. <EOE> <BOE> Feeling daring, the goose decides to fly to the city, intrigued by the exciting things he has heard. <EOE> <BOE> Upon arriving in the city, the goose is shocked to find everything very different from what he expected. <EOE> <BOE> The goose hears a voice calling him and discovers a small boy who greets him warmly. <EOE> <BOE> The boy offers the goose a treat, and the curious goose happily accepts. <EOE> <BOE> The boy takes the goose to a nearby park and spoils him with delicious goodies, bringing the goose immense joy. <EOE> <BOE> Grateful for the boy's kindness, the goose thanks him before soaring back into the sky. <EOE> <END_OUTLINE> <START_STORY> Once upon a time, there was an incredible goose. He used to fly up high in the sky and check out the world below. One day, he was feeling extra daring so he flew all the way to the city. He had heard about all the exciting things he could find there. But when he arrived, he was shocked to find everything was very different. [MASK_START][MASK_END] The boy asked the goose if he wanted a treat. The goose was curious and said yes!. The boy took him to a nearby park and proceeded to spoil him with lots of goodies. The goose was overjoyed with joy. He couldn't believe his incredible luck. He thanked the boy before taking off into the sky. And that's how the incredible goose was spoiled by a kind little boy. <END_STORY>\n",
      "\n",
      "--- Predicted Masked Sentence ---\n",
      "\n",
      "The goose heard a voice calling him. One day, he saw a small boy. The boy was very kind and said, \"Hi, goose!. I can't wait to see you!\" The goose smiled and smiled.\n",
      "\n",
      "--- Target Sentence ---\n",
      "\n",
      "Suddenly he heard a voice calling out to him. \"Hello my feathered friend,\" it said. He looked down to see a small boy standing there.\n",
      "\n",
      "--- Input Example ---\n",
      "\n",
      "<DELETE> <START_OUTLINE> <BOE> Tom, a painter, enjoys creating fancy pictures. <EOE> <BOE> While painting a big, pretty flower, Tom is approached by a little girl named Sue. <EOE> <BOE> Sue begins to paint but accidentally makes a mess, ruining the flower. <EOE> <BOE> Both Tom and Sue feel sad about the ruined flower and want to fix it. <EOE> <BOE> Together, they work to restore the flower, making it even more beautiful than before. <EOE> <BOE> In the end, Tom and Sue are happy and have formed a good friendship. <EOE> <END_OUTLINE> <START_STORY> Once upon a time, there was a painter named Tom. Tom liked to paint fancy pictures. One day, he was painting a big, pretty flower. A little girl named Sue saw Tom painting. [MASK_START][MASK_END] The fancy flower was not pretty anymore. Tom was sad, and Sue was sorry. They both wanted to fix the flower. Tom said, \"Let's try again, please.\" They worked together to make the flower pretty again. In the end, the fancy flower was even more beautiful than before. Tom and Sue were happy, and they became good friends. <END_STORY>\n",
      "\n",
      "--- Predicted Masked Sentence ---\n",
      "\n",
      "Sue started to paint, but he accidentally made a mess. The flower was not pretty anymore. Tom and Sue wanted to fix the flower, but they did not know what to do.\n",
      "\n",
      "--- Target Sentence ---\n",
      "\n",
      "She wanted to paint too. She asked, \"Can I paint too, please?\" Tom said, \"Yes, but be careful.\" Sue started to paint, but she made a mess.\n"
     ]
    }
   ],
   "source": [
    "# More test predictions\n",
    "\n",
    "for i in range(5):\n",
    "  test_example = test_dataset[i][\"input_text\"]\n",
    "  predicted = generate_masked_span(model, tokenizer, test_example)\n",
    "  print(\"\\n--- Input Example ---\\n\")\n",
    "  print(test_example)\n",
    "  print(\"\\n--- Predicted Masked Sentence ---\\n\")\n",
    "  print(predicted)\n",
    "  print(\"\\n--- Target Sentence ---\\n\")\n",
    "  print(test_dataset[i][\"target_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def remove_mask_content(example):\n",
    "    text = example[\"input_text\"]\n",
    "    example[\"input_text\"] = re.sub(\n",
    "        r\"\\[MASK_START\\].*?\\[MASK_END\\]\",\n",
    "        \"[MASK_START][MASK_END]\",\n",
    "        text,\n",
    "        flags=re.DOTALL,\n",
    "    )\n",
    "    return example\n",
    "\n",
    "\n",
    "def make_preprocess_function(tokenizer, max_input_length, max_target_length):\n",
    "    \"\"\"\n",
    "    Factory that returns a preprocess_function configured with\n",
    "    the chosen max_input_length and max_target_length.\n",
    "    \"\"\"\n",
    "    def preprocess_function(batch):\n",
    "        # Encoder inputs\n",
    "        model_inputs = tokenizer(\n",
    "            batch[\"input_text\"],\n",
    "            max_length=max_input_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        # Decoder targets\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(\n",
    "                batch[\"target_text\"],\n",
    "                max_length=max_target_length,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "            )\n",
    "\n",
    "        # Replace pad token IDs with -100 so they’re ignored in loss\n",
    "        labels[\"input_ids\"] = [\n",
    "            [\n",
    "                (token if token != tokenizer.pad_token_id else -100)\n",
    "                for token in label\n",
    "            ]\n",
    "            for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "    return preprocess_function\n",
    "\n",
    "\n",
    "def prepare_dataloaders(\n",
    "    data_path: str = \"masked_dataset\",\n",
    "    model_name: str = \"t5-small\",\n",
    "    subset_frac: float = 0.25,\n",
    "    max_input_length: int = 512,\n",
    "    max_target_length: int = 128,\n",
    "    batch_size: int = 4,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads the dataset, applies cleaning, takes a subset of the train split,\n",
    "    tokenizes, and returns DataLoaders (plus tokenizer & tokenized datasets).\n",
    "    \"\"\"\n",
    "\n",
    "    # Load dataset and split\n",
    "    full_dataset = load_from_disk(data_path)\n",
    "    train_test = full_dataset.train_test_split(test_size=0.1, seed=seed)\n",
    "    train_dataset = train_test[\"train\"]\n",
    "    test_dataset = train_test[\"test\"]\n",
    "\n",
    "    # Clean mask content\n",
    "    train_dataset = train_dataset.map(remove_mask_content)\n",
    "    test_dataset = test_dataset.map(remove_mask_content)\n",
    "\n",
    "    # Subset 25% of the training data (after cleaning)\n",
    "    if 0 < subset_frac < 1.0:\n",
    "        train_dataset = train_dataset.shuffle(seed=seed)\n",
    "        subset_size = int(len(train_dataset) * subset_frac)\n",
    "        train_dataset = train_dataset.select(range(subset_size))\n",
    "\n",
    "    # Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Preprocessing function with chosen lengths\n",
    "    preprocess_fn = make_preprocess_function(\n",
    "        tokenizer,\n",
    "        max_input_length=max_input_length,\n",
    "        max_target_length=max_target_length,\n",
    "    )\n",
    "\n",
    "    # Tokenize train and test\n",
    "    tokenized_train = train_dataset.map(\n",
    "        preprocess_fn,\n",
    "        batched=True,\n",
    "        remove_columns=train_dataset.column_names,\n",
    "    )\n",
    "    tokenized_test = test_dataset.map(\n",
    "        preprocess_fn,\n",
    "        batched=True,\n",
    "        remove_columns=test_dataset.column_names,\n",
    "    )\n",
    "\n",
    "    # Set torch format\n",
    "    tokenized_train.set_format(\n",
    "        type=\"torch\",\n",
    "        columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    )\n",
    "    tokenized_test.set_format(\n",
    "        type=\"torch\",\n",
    "        columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    )\n",
    "\n",
    "    # Dataloaders\n",
    "    train_loader = DataLoader(tokenized_train, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(tokenized_test, batch_size=batch_size)\n",
    "\n",
    "    return {\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"train_loader\": train_loader,\n",
    "        \"test_loader\": test_loader,\n",
    "        \"tokenized_train\": tokenized_train,\n",
    "        \"tokenized_test\": tokenized_test,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 0 BaseLine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 [Train]: 100%|██████████| 1292/1292 [04:29<00:00,  4.79it/s, batch_loss=1.86]\n",
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 | Train Loss: 2.2187 | Val Loss: 1.8843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 [Train]: 100%|██████████| 1292/1292 [04:30<00:00,  4.78it/s, batch_loss=1.51]\n",
      "                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 | Train Loss: 1.9613 | Val Loss: 1.7831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Core configs for Stage 0\n",
    "MODEL_NAME = \"t5-small\"\n",
    "LR = 3e-5\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 2\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Prepare data (25% train subset, 512/128 lengths)\n",
    "data = prepare_dataloaders(\n",
    "    data_path=\"masked_dataset\",\n",
    "    model_name=MODEL_NAME,\n",
    "    subset_frac=0.25,\n",
    "    max_input_length=512,\n",
    "    max_target_length=128,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=42,\n",
    ")\n",
    "tokenizer = data[\"tokenizer\"]\n",
    "train_loader = data[\"train_loader\"]\n",
    "test_loader = data[\"test_loader\"]\n",
    "\n",
    "# Model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "# -----------------------------\n",
    "# Stage 0: Training + Validation Loop\n",
    "# -----------------------------\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # ---- Training ----\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "\n",
    "    train_pbar = tqdm(\n",
    "        train_loader,\n",
    "        desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\",\n",
    "        leave=True,\n",
    "    )\n",
    "\n",
    "    for batch in train_pbar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        train_pbar.set_postfix({\"batch_loss\": loss.item()})\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(\n",
    "            test_loader,\n",
    "            desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Val]\",\n",
    "            leave=False,\n",
    "        )\n",
    "        for batch in val_pbar:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            val_pbar.set_postfix({\"batch_loss\": loss.item()})\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(test_loader)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{NUM_EPOCHS} | \"\n",
    "        f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "        f\"Val Loss: {avg_val_loss:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to stage0_baseline_model\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Save final model checkpoint\n",
    "# -----------------------------\n",
    "# save_path = \"stage0_baseline_model\"\n",
    "# model.save_pretrained(save_path)\n",
    "# tokenizer.save_pretrained(save_path)\n",
    "\n",
    "# print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1 LR Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================\n",
      "  Starting LR experiment: 1e-05\n",
      "============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR 1e-05 | Epoch 1/2 [Train]: 100%|██████████| 1292/1292 [04:30<00:00,  4.78it/s, loss=2.25]\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LR 1e-05] Epoch 1/2 | Train Loss: 2.4031 | Val Loss: 2.0461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR 1e-05 | Epoch 2/2 [Train]: 100%|██████████| 1292/1292 [04:30<00:00,  4.77it/s, loss=2.32]\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LR 1e-05] Epoch 2/2 | Train Loss: 2.1493 | Val Loss: 1.9317\n",
      "Saved model for LR=1e-05 to 'checkpoint_lr_1e-05'\n",
      "\n",
      "\n",
      "============================\n",
      "  Starting LR experiment: 5e-05\n",
      "============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR 5e-05 | Epoch 1/2 [Train]: 100%|██████████| 1292/1292 [04:30<00:00,  4.77it/s, loss=1.83]\n",
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LR 5e-05] Epoch 1/2 | Train Loss: 2.1369 | Val Loss: 1.8149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR 5e-05 | Epoch 2/2 [Train]: 100%|██████████| 1292/1292 [04:31<00:00,  4.75it/s, loss=1.97] \n",
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LR 5e-05] Epoch 2/2 | Train Loss: 1.8757 | Val Loss: 1.7110\n",
      "Saved model for LR=5e-05 to 'checkpoint_lr_5e-05'\n",
      "\n",
      "\n",
      "============================\n",
      "  Starting LR experiment: 0.0001\n",
      "============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR 0.0001 | Epoch 1/2 [Train]: 100%|██████████| 1292/1292 [04:31<00:00,  4.76it/s, loss=2.4]  \n",
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LR 0.0001] Epoch 1/2 | Train Loss: 2.0448 | Val Loss: 1.7309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR 0.0001 | Epoch 2/2 [Train]: 100%|██████████| 1292/1292 [04:30<00:00,  4.77it/s, loss=1.28] \n",
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LR 0.0001] Epoch 2/2 | Train Loss: 1.7640 | Val Loss: 1.6220\n",
      "Saved model for LR=0.0001 to 'checkpoint_lr_0.0001'\n",
      "\n",
      "\n",
      "============================\n",
      "  Starting LR experiment: 0.001\n",
      "============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR 0.001 | Epoch 1/2 [Train]: 100%|██████████| 1292/1292 [04:31<00:00,  4.76it/s, loss=2.2]  \n",
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LR 0.001] Epoch 1/2 | Train Loss: 1.9597 | Val Loss: 1.6397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR 0.001 | Epoch 2/2 [Train]: 100%|██████████| 1292/1292 [04:30<00:00,  4.77it/s, loss=1.7]  \n",
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LR 0.001] Epoch 2/2 | Train Loss: 1.5781 | Val Loss: 1.5512\n",
      "Saved model for LR=0.001 to 'checkpoint_lr_0.001'\n",
      "\n",
      "\n",
      "============================\n",
      "  Starting LR experiment: 0.01\n",
      "============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR 0.01 | Epoch 1/2 [Train]: 100%|██████████| 1292/1292 [04:30<00:00,  4.77it/s, loss=3.43]\n",
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LR 0.01] Epoch 1/2 | Train Loss: 3.2018 | Val Loss: 2.9429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR 0.01 | Epoch 2/2 [Train]: 100%|██████████| 1292/1292 [04:30<00:00,  4.78it/s, loss=4.93]\n",
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LR 0.01] Epoch 2/2 | Train Loss: 3.7458 | Val Loss: 4.5052\n",
      "Saved model for LR=0.01 to 'checkpoint_lr_0.01'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Stage 1: Learning Rate Sweep\n",
    "# -----------------------------\n",
    "\n",
    "LR_LIST = [1e-5, 5e-5, 1e-4, 1e-3, 1e-2]\n",
    "NUM_EPOCHS = 2                     \n",
    "BATCH_SIZE = 4\n",
    "MODEL_NAME = \"t5-small\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Prepare dataloaders once (no need to redo for each LR)\n",
    "data = prepare_dataloaders(\n",
    "    data_path=\"masked_dataset\",\n",
    "    model_name=MODEL_NAME,\n",
    "    subset_frac=0.25,\n",
    "    max_input_length=512,\n",
    "    max_target_length=128,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=42,\n",
    ")\n",
    "tokenizer = data[\"tokenizer\"]\n",
    "train_loader = data[\"train_loader\"]\n",
    "test_loader = data[\"test_loader\"]\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "for lr in LR_LIST:\n",
    "    print(f\"\\n============================\")\n",
    "    print(f\"  Starting LR experiment: {lr}\")\n",
    "    print(f\"============================\\n\")\n",
    "\n",
    "    # --- Reinitialize the model for every LR ---\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    # --- Train + validate for NUM_EPOCHS ---\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "        # ----- Training -----\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"LR {lr} | Epoch {epoch+1}/{NUM_EPOCHS} [Train]\")\n",
    "        for batch in pbar:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # ----- Validation -----\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            vbar = tqdm(test_loader, desc=f\"LR {lr} | Epoch {epoch+1}/{NUM_EPOCHS} [Val]\", leave=False)\n",
    "            for batch in vbar:\n",
    "                input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "                attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "                labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels,\n",
    "                )\n",
    "                total_val_loss += outputs.loss.item()\n",
    "                vbar.set_postfix({\"loss\": outputs.loss.item()})\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(test_loader)\n",
    "\n",
    "        print(f\"[LR {lr}] Epoch {epoch+1}/{NUM_EPOCHS} | \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # ----- Save checkpoint for this LR -----\n",
    "    # save_dir = f\"checkpoint_lr_{lr}\"\n",
    "    # model.save_pretrained(save_dir)\n",
    "    # tokenizer.save_pretrained(save_dir)\n",
    "    # print(f\"Saved model for LR={lr} to '{save_dir}'\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2 Batch Size Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================\n",
      "  Starting batch size experiment: 4\n",
      "====================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BS 4 | Epoch 1/2 [Train]: 100%|██████████| 1292/1292 [04:29<00:00,  4.80it/s, loss=2.04]\n",
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BS 4] Epoch 1/2 | Train Loss: 1.9635 | Val Loss: 1.6554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BS 4 | Epoch 2/2 [Train]: 100%|██████████| 1292/1292 [04:30<00:00,  4.77it/s, loss=1.44] \n",
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BS 4] Epoch 2/2 | Train Loss: 1.5883 | Val Loss: 1.5584\n",
      "Saved model for batch_size=4 to 'checkpoint_bs_4'\n",
      "\n",
      "\n",
      "====================================\n",
      "  Starting batch size experiment: 6\n",
      "====================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BS 6 | Epoch 1/2 [Train]: 100%|██████████| 862/862 [07:22<00:00,  1.95it/s, loss=1.44] \n",
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BS 6] Epoch 1/2 | Train Loss: 1.9487 | Val Loss: 1.6574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BS 6 | Epoch 2/2 [Train]: 100%|██████████| 862/862 [07:15<00:00,  1.98it/s, loss=1.88] \n",
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BS 6] Epoch 2/2 | Train Loss: 1.5741 | Val Loss: 1.5340\n",
      "Saved model for batch_size=6 to 'checkpoint_bs_6'\n",
      "\n",
      "\n",
      "====================================\n",
      "  Starting batch size experiment: 8\n",
      "====================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BS 8 | Epoch 1/2 [Train]: 100%|██████████| 646/646 [18:04<00:00,  1.68s/it, loss=1.7] \n",
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BS 8] Epoch 1/2 | Train Loss: 1.9518 | Val Loss: 1.6295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BS 8 | Epoch 2/2 [Train]: 100%|██████████| 646/646 [18:30<00:00,  1.72s/it, loss=1.18] \n",
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BS 8] Epoch 2/2 | Train Loss: 1.5779 | Val Loss: 1.5356\n",
      "Saved model for batch_size=8 to 'checkpoint_bs_8'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Stage 2: Batch Size Sweep\n",
    "# -----------------------------\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "MODEL_NAME = \"t5-small\"\n",
    "LR = 1e-3                 # chosen from Stage 1\n",
    "BATCH_SIZE_LIST = [4, 6, 8]   \n",
    "NUM_EPOCHS = 2\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "for batch_size in BATCH_SIZE_LIST:\n",
    "    print(\"\\n====================================\")\n",
    "    print(f\"  Starting batch size experiment: {batch_size}\")\n",
    "    print(\"====================================\\n\")\n",
    "\n",
    "    # Prepare data for this batch size (25% subset, same lengths)\n",
    "    data = prepare_dataloaders(\n",
    "        data_path=\"masked_dataset\",\n",
    "        model_name=MODEL_NAME,\n",
    "        subset_frac=0.25,\n",
    "        max_input_length=512,\n",
    "        max_target_length=128,\n",
    "        batch_size=batch_size,\n",
    "        seed=42,\n",
    "    )\n",
    "    tokenizer = data[\"tokenizer\"]\n",
    "    train_loader = data[\"train_loader\"]\n",
    "    test_loader = data[\"test_loader\"]\n",
    "\n",
    "    # Model & optimizer (re-init for each batch size)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "    optimizer = AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        # ----- Training -----\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "\n",
    "        train_pbar = tqdm(\n",
    "            train_loader,\n",
    "            desc=f\"BS {batch_size} | Epoch {epoch+1}/{NUM_EPOCHS} [Train]\",\n",
    "            leave=True,\n",
    "        )\n",
    "\n",
    "        for batch in train_pbar:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            train_pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # ----- Validation -----\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(\n",
    "                test_loader,\n",
    "                desc=f\"BS {batch_size} | Epoch {epoch+1}/{NUM_EPOCHS} [Val]\",\n",
    "                leave=False,\n",
    "            )\n",
    "            for batch in val_pbar:\n",
    "                input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "                attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "                labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels,\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "                total_val_loss += loss.item()\n",
    "                val_pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(test_loader)\n",
    "\n",
    "        print(\n",
    "            f\"[BS {batch_size}] Epoch {epoch+1}/{NUM_EPOCHS} | \"\n",
    "            f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "            f\"Val Loss: {avg_val_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "    # ----- Save checkpoint for this batch size -----\n",
    "    # save_dir = f\"checkpoint_bs_{batch_size}\"\n",
    "    # model.save_pretrained(save_dir)\n",
    "    # tokenizer.save_pretrained(save_dir)\n",
    "    # print(f\"Saved model for batch_size={batch_size} to '{save_dir}'\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 3 Context Length Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================\n",
      "  Starting context experiment: input=256, target=64\n",
      "====================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\e0979790\\AppData\\Local\\anaconda3\\envs\\nlp311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CTX in=256, tgt=64 | Epoch 1/2 [Train]: 100%|██████████| 1292/1292 [06:12<00:00,  3.47it/s, loss=1.29] \n",
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CTX in=256, tgt=64] Epoch 1/2 | Train Loss: 2.0172 | Val Loss: 1.7111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CTX in=256, tgt=64 | Epoch 2/2 [Train]: 100%|██████████| 1292/1292 [06:02<00:00,  3.56it/s, loss=1.08] \n",
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CTX in=256, tgt=64] Epoch 2/2 | Train Loss: 1.6402 | Val Loss: 1.6247\n",
      "Saved model for context (in=256, tgt=64) to 'checkpoint_ctx_in256_tgt64'\n",
      "\n",
      "\n",
      "====================================\n",
      "  Starting context experiment: input=384, target=96\n",
      "====================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CTX in=384, tgt=96 | Epoch 1/2 [Train]: 100%|██████████| 1292/1292 [03:26<00:00,  6.26it/s, loss=1.43] \n",
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CTX in=384, tgt=96] Epoch 1/2 | Train Loss: 1.9733 | Val Loss: 1.6703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CTX in=384, tgt=96 | Epoch 2/2 [Train]: 100%|██████████| 1292/1292 [03:25<00:00,  6.28it/s, loss=1.27] \n",
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CTX in=384, tgt=96] Epoch 2/2 | Train Loss: 1.5872 | Val Loss: 1.5662\n",
      "Saved model for context (in=384, tgt=96) to 'checkpoint_ctx_in384_tgt96'\n",
      "\n",
      "\n",
      "====================================\n",
      "  Starting context experiment: input=512, target=128\n",
      "====================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CTX in=512, tgt=128 | Epoch 1/2 [Train]: 100%|██████████| 1292/1292 [04:31<00:00,  4.76it/s, loss=1.37] \n",
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CTX in=512, tgt=128] Epoch 1/2 | Train Loss: 1.9669 | Val Loss: 1.6796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CTX in=512, tgt=128 | Epoch 2/2 [Train]: 100%|██████████| 1292/1292 [04:31<00:00,  4.76it/s, loss=2.08] \n",
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CTX in=512, tgt=128] Epoch 2/2 | Train Loss: 1.5891 | Val Loss: 1.5520\n",
      "Saved model for context (in=512, tgt=128) to 'checkpoint_ctx_in512_tgt128'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Stage 3: Context Length Sweep\n",
    "# -----------------------------\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "MODEL_NAME = \"t5-small\"\n",
    "LR = 1e-3           # from Stage 1\n",
    "BATCH_SIZE = 4      # from Stage 2\n",
    "NUM_EPOCHS = 2\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# (max_input_length, max_target_length) configs to test\n",
    "CONTEXT_CONFIGS = [\n",
    "    (256, 64),\n",
    "    (384, 96),\n",
    "    (512, 128),\n",
    "]\n",
    "\n",
    "for max_in, max_tgt in CONTEXT_CONFIGS:\n",
    "    print(\"\\n====================================\")\n",
    "    print(f\"  Starting context experiment: input={max_in}, target={max_tgt}\")\n",
    "    print(\"====================================\\n\")\n",
    "\n",
    "    # Prepare data for this context length\n",
    "    data = prepare_dataloaders(\n",
    "        data_path=\"masked_dataset\",\n",
    "        model_name=MODEL_NAME,\n",
    "        subset_frac=0.25,\n",
    "        max_input_length=max_in,\n",
    "        max_target_length=max_tgt,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        seed=42,\n",
    "    )\n",
    "    tokenizer = data[\"tokenizer\"]\n",
    "    train_loader = data[\"train_loader\"]\n",
    "    test_loader = data[\"test_loader\"]\n",
    "\n",
    "    # Model & optimizer (re-init for each config)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "    optimizer = AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        # ----- Training -----\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "\n",
    "        train_pbar = tqdm(\n",
    "            train_loader,\n",
    "            desc=f\"CTX in={max_in}, tgt={max_tgt} | Epoch {epoch+1}/{NUM_EPOCHS} [Train]\",\n",
    "            leave=True,\n",
    "        )\n",
    "\n",
    "        for batch in train_pbar:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            train_pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # ----- Validation -----\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(\n",
    "                test_loader,\n",
    "                desc=f\"CTX in={max_in}, tgt={max_tgt} | Epoch {epoch+1}/{NUM_EPOCHS} [Val]\",\n",
    "                leave=False,\n",
    "            )\n",
    "            for batch in val_pbar:\n",
    "                input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "                attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "                labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels,\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "                total_val_loss += loss.item()\n",
    "                val_pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(test_loader)\n",
    "\n",
    "        print(\n",
    "            f\"[CTX in={max_in}, tgt={max_tgt}] Epoch {epoch+1}/{NUM_EPOCHS} | \"\n",
    "            f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "            f\"Val Loss: {avg_val_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "    # ----- Save checkpoint for this context config -----\n",
    "    # save_dir = f\"checkpoint_ctx_in{max_in}_tgt{max_tgt}\"\n",
    "    # model.save_pretrained(save_dir)\n",
    "    # tokenizer.save_pretrained(save_dir)\n",
    "    # print(f\"Saved model for context (in={max_in}, tgt={max_tgt}) to '{save_dir}'\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 4 Epoch Count and Weight Decay Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================\n",
      "  Epoch/WD experiment: epochs=2, weight_decay=0.0\n",
      "====================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WD=0.0 | Epoch 1/2 [Train]: 100%|██████████| 1292/1292 [03:20<00:00,  6.43it/s, loss=1.52] \n",
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WD=0.0, Epoch 1/2] | Train Loss: 1.9738 | Val Loss: 1.6793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WD=0.0 | Epoch 2/2 [Train]: 100%|██████████| 1292/1292 [03:21<00:00,  6.41it/s, loss=1.76] \n",
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WD=0.0, Epoch 2/2] | Train Loss: 1.5967 | Val Loss: 1.5823\n",
      "Saved model to 'checkpoint_in384_tgt96_ep2_wd0p0'\n",
      "\n",
      "\n",
      "====================================\n",
      "  Epoch/WD experiment: epochs=3, weight_decay=0.0\n",
      "====================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WD=0.0 | Epoch 1/3 [Train]: 100%|██████████| 1292/1292 [03:22<00:00,  6.37it/s, loss=2.04] \n",
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WD=0.0, Epoch 1/3] | Train Loss: 1.9828 | Val Loss: 1.6752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WD=0.0 | Epoch 2/3 [Train]: 100%|██████████| 1292/1292 [03:22<00:00,  6.38it/s, loss=1.21] \n",
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WD=0.0, Epoch 2/3] | Train Loss: 1.5988 | Val Loss: 1.5591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WD=0.0 | Epoch 3/3 [Train]: 100%|██████████| 1292/1292 [03:22<00:00,  6.39it/s, loss=1.51] \n",
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WD=0.0, Epoch 3/3] | Train Loss: 1.4002 | Val Loss: 1.5202\n",
      "Saved model to 'checkpoint_in384_tgt96_ep3_wd0p0'\n",
      "\n",
      "\n",
      "====================================\n",
      "  Epoch/WD experiment: epochs=4, weight_decay=0.0\n",
      "====================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WD=0.0 | Epoch 1/4 [Train]: 100%|██████████| 1292/1292 [03:23<00:00,  6.36it/s, loss=2.18] \n",
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WD=0.0, Epoch 1/4] | Train Loss: 1.9699 | Val Loss: 1.6684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WD=0.0 | Epoch 2/4 [Train]: 100%|██████████| 1292/1292 [03:23<00:00,  6.36it/s, loss=1.37] \n",
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WD=0.0, Epoch 2/4] | Train Loss: 1.5968 | Val Loss: 1.5774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WD=0.0 | Epoch 3/4 [Train]: 100%|██████████| 1292/1292 [03:22<00:00,  6.38it/s, loss=2.11] \n",
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WD=0.0, Epoch 3/4] | Train Loss: 1.3989 | Val Loss: 1.5294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WD=0.0 | Epoch 4/4 [Train]: 100%|██████████| 1292/1292 [03:22<00:00,  6.37it/s, loss=2.08] \n",
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WD=0.0, Epoch 4/4] | Train Loss: 1.2537 | Val Loss: 1.5049\n",
      "Saved model to 'checkpoint_in384_tgt96_ep4_wd0p0'\n",
      "\n",
      "\n",
      "====================================\n",
      "  Epoch/WD experiment: epochs=2, weight_decay=0.01\n",
      "====================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WD=0.01 | Epoch 1/2 [Train]: 100%|██████████| 1292/1292 [03:25<00:00,  6.27it/s, loss=2.2]  \n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WD=0.01, Epoch 1/2] | Train Loss: 1.9685 | Val Loss: 1.6617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WD=0.01 | Epoch 2/2 [Train]: 100%|██████████| 1292/1292 [03:26<00:00,  6.25it/s, loss=1.89] \n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WD=0.01, Epoch 2/2] | Train Loss: 1.5978 | Val Loss: 1.5823\n",
      "Saved model to 'checkpoint_in384_tgt96_ep2_wd0p01'\n",
      "\n",
      "\n",
      "====================================\n",
      "  Epoch/WD experiment: epochs=3, weight_decay=0.01\n",
      "====================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WD=0.01 | Epoch 1/3 [Train]: 100%|██████████| 1292/1292 [03:25<00:00,  6.28it/s, loss=1.94] \n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WD=0.01, Epoch 1/3] | Train Loss: 1.9844 | Val Loss: 1.6791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WD=0.01 | Epoch 2/3 [Train]: 100%|██████████| 1292/1292 [03:25<00:00,  6.29it/s, loss=1.84] \n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WD=0.01, Epoch 2/3] | Train Loss: 1.5974 | Val Loss: 1.5714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WD=0.01 | Epoch 3/3 [Train]: 100%|██████████| 1292/1292 [03:26<00:00,  6.25it/s, loss=1.17] \n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WD=0.01, Epoch 3/3] | Train Loss: 1.3915 | Val Loss: 1.5400\n",
      "Saved model to 'checkpoint_in384_tgt96_ep3_wd0p01'\n",
      "\n",
      "\n",
      "====================================\n",
      "  Epoch/WD experiment: epochs=4, weight_decay=0.01\n",
      "====================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WD=0.01 | Epoch 1/4 [Train]: 100%|██████████| 1292/1292 [03:27<00:00,  6.22it/s, loss=2.36] \n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WD=0.01, Epoch 1/4] | Train Loss: 1.9744 | Val Loss: 1.6690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WD=0.01 | Epoch 2/4 [Train]: 100%|██████████| 1292/1292 [03:27<00:00,  6.23it/s, loss=1.8]  \n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WD=0.01, Epoch 2/4] | Train Loss: 1.5941 | Val Loss: 1.5814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WD=0.01 | Epoch 3/4 [Train]: 100%|██████████| 1292/1292 [03:27<00:00,  6.22it/s, loss=0.832]\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WD=0.01, Epoch 3/4] | Train Loss: 1.4007 | Val Loss: 1.5249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WD=0.01 | Epoch 4/4 [Train]: 100%|██████████| 1292/1292 [03:26<00:00,  6.26it/s, loss=1.1]  \n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WD=0.01, Epoch 4/4] | Train Loss: 1.2494 | Val Loss: 1.5095\n",
      "Saved model to 'checkpoint_in384_tgt96_ep4_wd0p01'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Stage 4: Epoch & Weight Decay Sweep\n",
    "# -----------------------------\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "MODEL_NAME = \"t5-small\"\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 4\n",
    "MAX_INPUT_LENGTH = 384\n",
    "MAX_TARGET_LENGTH = 96\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Epoch and weight decay configs to test\n",
    "EPOCH_OPTIONS = [2, 3, 4]\n",
    "WEIGHT_DECAY_OPTIONS = [0.0, 0.01]\n",
    "\n",
    "# Prepare data once for this context & batch size\n",
    "data = prepare_dataloaders(\n",
    "    data_path=\"masked_dataset\",\n",
    "    model_name=MODEL_NAME,\n",
    "    subset_frac=0.25,\n",
    "    max_input_length=MAX_INPUT_LENGTH,\n",
    "    max_target_length=MAX_TARGET_LENGTH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=42,\n",
    ")\n",
    "tokenizer = data[\"tokenizer\"]\n",
    "train_loader = data[\"train_loader\"]\n",
    "test_loader = data[\"test_loader\"]\n",
    "\n",
    "for wd in WEIGHT_DECAY_OPTIONS:\n",
    "    for num_epochs in EPOCH_OPTIONS:\n",
    "        print(\"\\n====================================\")\n",
    "        print(f\"  Epoch/WD experiment: epochs={num_epochs}, weight_decay={wd}\")\n",
    "        print(\"====================================\\n\")\n",
    "\n",
    "        # Re-init model & optimizer for this config\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "        optimizer = AdamW(model.parameters(), lr=LR, weight_decay=wd)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            # ----- Training -----\n",
    "            model.train()\n",
    "            total_train_loss = 0.0\n",
    "\n",
    "            train_pbar = tqdm(\n",
    "                train_loader,\n",
    "                desc=f\"WD={wd} | Epoch {epoch+1}/{num_epochs} [Train]\",\n",
    "                leave=True,\n",
    "            )\n",
    "\n",
    "            for batch in train_pbar:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "                attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "                labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels,\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_train_loss += loss.item()\n",
    "                train_pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "            avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "            # ----- Validation -----\n",
    "            model.eval()\n",
    "            total_val_loss = 0.0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                val_pbar = tqdm(\n",
    "                    test_loader,\n",
    "                    desc=f\"WD={wd} | Epoch {epoch+1}/{num_epochs} [Val]\",\n",
    "                    leave=False,\n",
    "                )\n",
    "                for batch in val_pbar:\n",
    "                    input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "                    attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "                    labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "                    outputs = model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        labels=labels,\n",
    "                    )\n",
    "                    loss = outputs.loss\n",
    "                    total_val_loss += loss.item()\n",
    "                    val_pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "            avg_val_loss = total_val_loss / len(test_loader)\n",
    "\n",
    "            print(\n",
    "                f\"[WD={wd}, Epoch {epoch+1}/{num_epochs}] | \"\n",
    "                f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "                f\"Val Loss: {avg_val_loss:.4f}\"\n",
    "            )\n",
    "\n",
    "        # Save checkpoint for this (epochs, weight decay) config\n",
    "        # wd_str = str(wd).replace(\".\", \"p\")\n",
    "        # save_dir = f\"checkpoint_in{MAX_INPUT_LENGTH}_tgt{MAX_TARGET_LENGTH}_ep{num_epochs}_wd{wd_str}\"\n",
    "        # model.save_pretrained(save_dir)\n",
    "        # tokenizer.save_pretrained(save_dir)\n",
    "        # print(f\"Saved model to '{save_dir}'\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 5 Scheduler Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================\n",
      "  Scheduler experiment: linear\n",
      "====================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "linear | Epoch 1/2 [Train]: 100%|██████████| 1292/1292 [03:22<00:00,  6.39it/s, loss=1.68] \n",
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[linear] Epoch 1/2 | Train Loss: 2.0003 | Val Loss: 1.6377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "linear | Epoch 2/2 [Train]: 100%|██████████| 1292/1292 [03:22<00:00,  6.37it/s, loss=1.41] \n",
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[linear] Epoch 2/2 | Train Loss: 1.5317 | Val Loss: 1.5182\n",
      "Saved model with scheduler='linear' to 'checkpoint_sched_linear_in384_tgt96'\n",
      "\n",
      "\n",
      "====================================\n",
      "  Scheduler experiment: cosine\n",
      "====================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cosine | Epoch 1/2 [Train]: 100%|██████████| 1292/1292 [03:21<00:00,  6.41it/s, loss=2.2]  \n",
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cosine] Epoch 1/2 | Train Loss: 1.9903 | Val Loss: 1.6264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cosine | Epoch 2/2 [Train]: 100%|██████████| 1292/1292 [03:21<00:00,  6.41it/s, loss=1.16] \n",
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cosine] Epoch 2/2 | Train Loss: 1.5040 | Val Loss: 1.5240\n",
      "Saved model with scheduler='cosine' to 'checkpoint_sched_cosine_in384_tgt96'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Stage 5: Scheduler Comparison\n",
    "# -----------------------------\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, get_scheduler\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "MODEL_NAME = \"t5-small\"\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 4\n",
    "MAX_INPUT_LENGTH = 384\n",
    "MAX_TARGET_LENGTH = 96\n",
    "NUM_EPOCHS = 2\n",
    "WEIGHT_DECAY = 0.0\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Prepare data once with the chosen config\n",
    "data = prepare_dataloaders(\n",
    "    data_path=\"masked_dataset\",\n",
    "    model_name=MODEL_NAME,\n",
    "    subset_frac=0.25,\n",
    "    max_input_length=MAX_INPUT_LENGTH,\n",
    "    max_target_length=MAX_TARGET_LENGTH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=42,\n",
    ")\n",
    "tokenizer = data[\"tokenizer\"]\n",
    "train_loader = data[\"train_loader\"]\n",
    "test_loader = data[\"test_loader\"]\n",
    "\n",
    "# Schedulers to test (compared against constant LR baseline you already ran)\n",
    "SCHEDULER_TYPES = [\"linear\", \"cosine\"]\n",
    "\n",
    "for sched_name in SCHEDULER_TYPES:\n",
    "    print(\"\\n====================================\")\n",
    "    print(f\"  Scheduler experiment: {sched_name}\")\n",
    "    print(\"====================================\\n\")\n",
    "\n",
    "    # Re-init model & optimizer\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "    optimizer = AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    # Total steps = epochs * steps per epoch\n",
    "    num_update_steps_per_epoch = len(train_loader)\n",
    "    num_training_steps = NUM_EPOCHS * num_update_steps_per_epoch\n",
    "    warmup_steps = int(0.1 * num_training_steps)  # 10% warmup\n",
    "\n",
    "    scheduler = get_scheduler(\n",
    "        name=sched_name,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        # ----- Training -----\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "\n",
    "        train_pbar = tqdm(\n",
    "            train_loader,\n",
    "            desc=f\"{sched_name} | Epoch {epoch+1}/{NUM_EPOCHS} [Train]\",\n",
    "            leave=True,\n",
    "        )\n",
    "\n",
    "        for batch in train_pbar:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()     # update LR\n",
    "            global_step += 1\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            train_pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # ----- Validation -----\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(\n",
    "                test_loader,\n",
    "                desc=f\"{sched_name} | Epoch {epoch+1}/{NUM_EPOCHS} [Val]\",\n",
    "                leave=False,\n",
    "            )\n",
    "            for batch in val_pbar:\n",
    "                input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "                attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "                labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels,\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "                total_val_loss += loss.item()\n",
    "                val_pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(test_loader)\n",
    "\n",
    "        print(\n",
    "            f\"[{sched_name}] Epoch {epoch+1}/{NUM_EPOCHS} | \"\n",
    "            f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "            f\"Val Loss: {avg_val_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "    # Save checkpoint for this scheduler\n",
    "    # save_dir = f\"checkpoint_sched_{sched_name}_in{MAX_INPUT_LENGTH}_tgt{MAX_TARGET_LENGTH}\"\n",
    "    # model.save_pretrained(save_dir)\n",
    "    # tokenizer.save_pretrained(save_dir)\n",
    "    # print(f\"Saved model with scheduler='{sched_name}' to '{save_dir}'\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 6 Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Epoch 1/10 ==========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]: 100%|██████████| 7182/7182 [24:51<00:00,  4.82it/s, loss=1.81] \n",
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 1.9227 | Val Loss: 1.5481\n",
      "✅ New best model saved to 'full_model_best' (val_loss=1.5481)\n",
      "\n",
      "========== Epoch 2/10 ==========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Train]: 100%|██████████| 7182/7182 [24:52<00:00,  4.81it/s, loss=1.34] \n",
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | Train Loss: 1.5571 | Val Loss: 1.3589\n",
      "✅ New best model saved to 'full_model_best' (val_loss=1.3589)\n",
      "\n",
      "========== Epoch 3/10 ==========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Train]: 100%|██████████| 7182/7182 [24:49<00:00,  4.82it/s, loss=1.82] \n",
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | Train Loss: 1.3364 | Val Loss: 1.2291\n",
      "✅ New best model saved to 'full_model_best' (val_loss=1.2291)\n",
      "\n",
      "========== Epoch 4/10 ==========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Train]: 100%|██████████| 7182/7182 [24:49<00:00,  4.82it/s, loss=1.69] \n",
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | Train Loss: 1.1697 | Val Loss: 1.1426\n",
      "✅ New best model saved to 'full_model_best' (val_loss=1.1426)\n",
      "\n",
      "========== Epoch 5/10 ==========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Train]: 100%|██████████| 7182/7182 [24:49<00:00,  4.82it/s, loss=0.931]\n",
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | Train Loss: 1.0314 | Val Loss: 1.0593\n",
      "✅ New best model saved to 'full_model_best' (val_loss=1.0593)\n",
      "\n",
      "========== Epoch 6/10 ==========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Train]: 100%|██████████| 7182/7182 [24:49<00:00,  4.82it/s, loss=0.861]\n",
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 | Train Loss: 0.9080 | Val Loss: 1.0041\n",
      "✅ New best model saved to 'full_model_best' (val_loss=1.0041)\n",
      "\n",
      "========== Epoch 7/10 ==========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Train]: 100%|██████████| 7182/7182 [24:53<00:00,  4.81it/s, loss=1.03] \n",
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 | Train Loss: 0.7992 | Val Loss: 0.9577\n",
      "✅ New best model saved to 'full_model_best' (val_loss=0.9577)\n",
      "\n",
      "========== Epoch 8/10 ==========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Train]: 100%|██████████| 7182/7182 [24:56<00:00,  4.80it/s, loss=0.644]\n",
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 | Train Loss: 0.7034 | Val Loss: 0.9107\n",
      "✅ New best model saved to 'full_model_best' (val_loss=0.9107)\n",
      "\n",
      "========== Epoch 9/10 ==========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Train]: 100%|██████████| 7182/7182 [24:50<00:00,  4.82it/s, loss=1.22] \n",
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 | Train Loss: 0.6247 | Val Loss: 0.8757\n",
      "✅ New best model saved to 'full_model_best' (val_loss=0.8757)\n",
      "\n",
      "========== Epoch 10/10 ==========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Train]: 100%|██████████| 7182/7182 [24:51<00:00,  4.82it/s, loss=0.394]\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 | Train Loss: 0.5662 | Val Loss: 0.8687\n",
      "✅ New best model saved to 'full_model_best' (val_loss=0.8687)\n",
      "\n",
      "Final model (last epoch) saved to 'Generator Models/full_model_best'\n",
      "Best validation loss achieved: 0.8687\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, get_scheduler\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -----------------------------\n",
    "# Full training config\n",
    "# -----------------------------\n",
    "MODEL_NAME = \"t5-small\"\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 4\n",
    "MAX_INPUT_LENGTH = 512\n",
    "MAX_TARGET_LENGTH = 128\n",
    "NUM_EPOCHS = 10\n",
    "WEIGHT_DECAY = 0.0\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "EARLY_STOPPING_PATIENCE = 3   # stop if no val improvement for N epochs\n",
    "WARMUP_RATIO = 0.1            # 10% warmup for linear scheduler\n",
    "\n",
    "# -----------------------------\n",
    "# Data: full dataset (no subset)\n",
    "# -----------------------------\n",
    "data = prepare_dataloaders(\n",
    "    data_path=\"masked_dataset_2\", # larger dataset\n",
    "    model_name=MODEL_NAME,\n",
    "    subset_frac=1.0,  # use 100% of training data\n",
    "    max_input_length=MAX_INPUT_LENGTH,\n",
    "    max_target_length=MAX_TARGET_LENGTH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=42,\n",
    ")\n",
    "tokenizer = data[\"tokenizer\"]\n",
    "train_loader = data[\"train_loader\"]\n",
    "val_loader = data[\"test_loader\"]   # treat this as validation\n",
    "\n",
    "# -----------------------------\n",
    "# Model, optimizer, scheduler\n",
    "# -----------------------------\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "optimizer = AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "num_update_steps_per_epoch = len(train_loader)\n",
    "num_training_steps = NUM_EPOCHS * num_update_steps_per_epoch\n",
    "num_warmup_steps = int(WARMUP_RATIO * num_training_steps)\n",
    "\n",
    "scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Training loop with early stopping\n",
    "# -----------------------------\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n========== Epoch {epoch+1}/{NUM_EPOCHS} ==========\\n\")\n",
    "\n",
    "    # ----- Training -----\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "\n",
    "    train_pbar = tqdm(\n",
    "        train_loader,\n",
    "        desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\",\n",
    "        leave=True,\n",
    "    )\n",
    "\n",
    "    for batch in train_pbar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        train_pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "    # ----- Validation -----\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(\n",
    "            val_loader,\n",
    "            desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Val]\",\n",
    "            leave=False,\n",
    "        )\n",
    "        for batch in val_pbar:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            total_val_loss += loss.item()\n",
    "            val_pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{NUM_EPOCHS} | \"\n",
    "        f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "        f\"Val Loss: {avg_val_loss:.4f}\"\n",
    "    )\n",
    "\n",
    "    # ----- Early stopping check -----\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_without_improvement = 0\n",
    "\n",
    "        # Save best model so far\n",
    "        best_save_dir = \"full_model_best\"\n",
    "        model.save_pretrained(best_save_dir)\n",
    "        tokenizer.save_pretrained(best_save_dir)\n",
    "        print(f\"✅ New best model saved to '{best_save_dir}' (val_loss={best_val_loss:.4f})\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        print(f\"No improvement for {epochs_without_improvement} epoch(s).\")\n",
    "\n",
    "        if epochs_without_improvement >= EARLY_STOPPING_PATIENCE:\n",
    "            print(\"⏹ Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# -----------------------------\n",
    "# Save final model (last epoch)\n",
    "# -----------------------------\n",
    "final_save_dir = \"full_model_best\"\n",
    "model.save_pretrained(final_save_dir)\n",
    "tokenizer.save_pretrained(final_save_dir)\n",
    "print(f\"\\nFinal model (last epoch) saved to '{final_save_dir}'\")\n",
    "print(f\"Best validation loss achieved: {best_val_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
